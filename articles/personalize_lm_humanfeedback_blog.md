---
title: "個人化されたフィードバックから学ぶLLMに関する論文を一緒に読みましょう！"
emoji: "🎯"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["LLM", "RLHF", "DPO", "Personalization", "User Modeling"]
published: false
---

# Personalized-RLHF (P-RLHF) — 個人化された人間フィードバックから学ぶLLMの新定式化
> この記事は，「自分の理解を深めたい」という気持ちで書いています．読者のみなさんと**同じ目線**で，一緒に理解を育てていくスタイルです．僕の理解が及ばない部分があれば，優しく教えていただけると幸いです！
> 著作権の関係で画像は掲載できないので，論文をぜひご一読ください！


# TL;DR

* **課題**：従来のRLHF（およびDPO）は“すべての人間の好みは同じ分布から来る”という**一様性仮定**を暗に置く。これは多数派の好みを“総意”として学習してしまい、少数派の好みを無視しがち。著者らはこの暗黙仮定を定式化し、**vanilla RLHF ≒ 多数決**であることを示す。
* **提案**：**P-RLHF**。軽量な**ユーザモデル**（明示テキストと暗黙好みの両方を埋め込み化）を**ベースLLMに結合**し、**個人化DPO（P-DPO）**で**ユーザ特化損失＋ユーザ非特化損失**を混合して共同学習する。
* **効果**：合成タスクでは、少数派には**長さ0の応答**を最適に出すなど、期待通りの個人化挙動を学習。実データ（**PRISM**）でも**P-DPOがvanilla DPOと“選択肢”を上回る勝率**（60%超）を示す。クラスタ型ユーザモデルが特に良好。
* **スケール**：ユーザモデルは**軽量**（パラメータ Nu ≪ LLMパラメータ Nl）。複数LLMを別々に訓練する方式より**計算・メモリ効率が高い**。



# なぜ必要か？— 一様性仮定の問題

* 論文は、vanilla RLHF/DPOが **“ユーザ一様性”** を暗に仮定していることを明示化：
  $P(y_1 \succ y_2 \mid x,u) = P(y_1 \succ y_2 \mid x)$
* これにより、学習された報酬/方策は**多数派の好みを全員の好みとみなす**＝**多数決**になることを示す（Lemma 3.2）。
* さらに、多数派が大きいほど、**少数派の好みは真の分布からのズレが大きくなる**（Lemma 3.3）。
* 結論：**多様・対立する好み**がある現実世界のデータでは、vanilla手法は**個人化に不向き**。



# 提案：P-RLHF 概要

## コンポーネント

* **ユーザモデル** $f_P$：ユーザ情報 $u=(u_t, u_p)$ から**ユーザ埋め込み $e_u$** を抽出。

  * **明示（explicit）**：テキスト情報 $u_t$ を**入力埋め込みそのもの**として利用。
  * **暗黙（implicit）**：ユーザID $u_p$ を**埋め込み列**にマップ（長さ $T_u$ が表現力を制御）。
* **埋め込みの結合**：$e_u = \text{concat}(e_u^{\mathrm{im}}, e_u^{\mathrm{ex}})$ を**ソフトプロンプティング**でベースLLMの**入力埋め込み前置**として与える。
* **個人化LLM $\pi_P$**：ベースLLM＋ユーザモデルで構成。

## 暗黙ユーザモデルの設計例

* **一様（Uniform）**：全員同じ埋め込み（=vanilla RLHFの仮定）。
* **個別（Individualized）**：共有成分 $e_0$ ＋各ユーザのオフセット $o_i$。
* **クラスタ（Cluster-based）**：クラスタ中心行列 $V$ と重み $w_i$ の**低ランク近似**としてユーザ埋め込みを表現（$K$ クラスタ）。
* **既知/未知ユーザへの挙動**：既知ユーザには $e_u^{\mathrm{ex}}, e_u^{\mathrm{im}}$ の両方を活用。未知ユーザ（またはテキスト情報なし）には**汎用の暗黙埋め込み $e_0$** のみで**非個人化応答**を生成。



# 学習：P-DPO（Personalized DPO）

* **損失**：**ユーザ特化**（IDあり）と**ユーザ非特化**（IDなし＝全IDを0に置換）の2項を**係数 $\alpha$** で混合：

  $$
  \min_{\pi_P}\;-\mathbb{E}_{(x,y_1,y_2,u_t,u_p)\sim D_P}\big[
  \alpha \log \sigma(\Delta_P) + (1-\alpha)\log \sigma(\Delta_{P,0})
  \big]
  $$

  ここで $\Delta_P = \beta \log \frac{\pi_P(y_1|x,u_t,u_p)}{\pi_{SFT}(y_1|x)} - \beta \log \frac{\pi_P(y_2|x,u_t,u_p)}{\pi_{SFT}(y_2|x)}$、
  $\Delta_{P,0}$ はユーザIDをすべて0に置いた同形式。
* **一般性**：この枠組みは**DPO以外の嗜好最適化**（例：IPO）にも拡張可能。
* **報酬解釈**：P-DPOは**暗黙報酬** $r_P(x,y,u)=\beta \log \frac{\pi_P(y|x,u)}{\pi_{SFT}(y|x)}$ を最大化する方策学習とみなせる。



# 実験ハイライト

## 1) **対立する好みの合成タスク（Reddit TL;DR）**

* セットアップ：作業者70%は**長文好み**、30%は**短文好み**（明確に対立）。上位10名を訓練に使用。

* 入力とペア情報
    * x：投稿本文、y₁/y₂：要約の二択、u：労働者ID（好みの“擬似真”を持つ）。学習は上位10名の労働者で実施

* 学習と比較条件
    * 学習法：P-DPO（個人化DPO）
    * 比較：SFT、vanilla DPO、汎用（generic）ユーザ埋め込みのP-DPO

* 評価指標
    * 要約の語数（平均±SE）で、誰にどの長さを出したかを確認
    * 付随して暗黙報酬の精度などの補助指標とアブレーションも報告

* **期待される最適挙動**：短文ユーザには**長さ0**の応答が最適（BTモデルの極限）、未知ユーザには**vanilla DPO相当の振る舞い**。


## 2) **指示追従 × 異なる好みプロファイル（Personalized-Soups）**

* GPT-4が6つのプロファイル（専門性・情報量・スタイルのA/B）で比較付与。P-DPOで検証。
  （データ詳細中心の記述のため、ここでは割愛。論文本体・付録を参照。）

* 入力とペア情報
* x：指示、（y₁,y₂）：Tulu-7Bの応答、GPT-4が3次元（専門性/情報量/スタイル）で付与した好み。明示の好みプロンプトはP-DPOには与えない（暗黙から学習）

* 学習と比較条件
    * 学習法：P-DPO（個別ユーザ前提、α=0.5、Tᵤ=10）。
    * 比較：Tulu-7B SFT、vanilla DPO、オラクル（真の好みプロンプトを明示）。

* 評価指標
    * **GPT-4ベースのペア勝率（win-rate）** で比較。

* **期待される結果**：専門性/情報量/スタイルといった多次元の“好み”に対しても、P-DPOが明示プロンプトなし（暗黙情報のみ）で各プロファイルに合わせた応答を作れるか

## 3) **実世界の大規模個人化データ（PRISM）**

* 1,500人／75か国、8,011会話、社会属性と自己申告の好み＋**細粒度なフィードバック**を含む**最大規模の個人化嗜好データ**。

* 入力とペア情報
    * x：対話履歴＋当該ターンのユーザ発話
    * uₜ：ユーザの属性・自己記述・好み
    * (y₁,y₂)：そのターンの**ユーザ選好（chosen/rejected）**ペア

* 学習と比較条件
    * SFT：Llama3-8B-Instructをベースに整備。
    * 学習法：P-DPO（個別／クラスタ K=10/100）。
    * 比較：vanilla DPO、Chosen responses（強力LLM群の人手選好出力）。

* 評価指標
    * GPT-4oのロールプレイ評価によるペア勝率（per-sample / per-user）。評価時はユーザ情報＋文脈を含む役割設定プロンプトを提示。

* **期待される結果**：明示テキストが無い/少ない場合でも暗黙情報だけで個人化効果が出るか、クラスタ型ユーザ表現が有効かを評価



# 結果（要約）

## 1) 合成タスク：相反する好み（Reddit TL;DR）

* **少数派ユーザ（短文嗜好）には長さ0**の応答を出力、**多数派には長文**——P-DPO が**理想挙動**を獲得。学習データに空応答は無かったのに、**外挿**で到達した点がポイント。
* **未知ユーザ**（テキスト情報なし）には、**generic埋め込み**で**vanilla DPO と同様の挙動**（=多数派寄り）に。

## 2) 多次元プロファイル：Personalized-Soups（指示追従）

* **P-DPO は SFT/vanilla DPO に約90%平均勝率**、一部プロファイルでは**100%勝利**。**“好みを明示したプロンプト（擬似オラクル）”**に対しても**平均70.24%勝率**（6ユーザ中5ユーザで59%超）。&#x20;
* 評価は Koala 50指示・GPT-4系フレームワークの**ペア比較 win-rate**。

## 3) 実世界：PRISM（1,500人・8,011会話）

* **P-DPO は vanilla DPO に全設定で >60% の勝率**：**個別(P-DPO)**、**クラスタK=10/100**のいずれも**per-sample 61.84–65.79%**/**per-user 60–65%**。&#x20;
* **“Chosen responses”**（強力LLM群の人手選好出力）に対しても **\~60% 勝率**。一方、**vanilla DPO は chosen に対し <50%**で**明示情報だけでは不十分**と示唆。
* **明示テキストなし（暗黙のみ）** でも個人化が有効：**20ユーザ**で **51.32%/55%**（per-sample/per-user）、**70ユーザ**に拡大すると **65.38%/72.86%**。
* **クラスタ型ユーザモデル**が最良：大規模ユーザ基盤に**低ランク近似**でよく伸びる。

## 4) スケールとアブレーション

* **Top40労働者**へ拡張しても、**P-DPO は高精度を維持**（Accuracy-average：**個別 91.94% / クラスタ 90.27%**；vanilla DPO **67.96%**）。
* **ユーザトークン長 $T_u$** は長いほど有利だが、**$T_u=1$ でも vanilla を大幅に上回る**。**$\alpha=1.0$** は見かけ向上する一方、**generic精度が不安定**→**ユーザ非特化項の併用が安定化に重要**。



# 限界・注意点

* **評価の信頼性**：PRISMでは**GPT-4oのロールプレイ評価**に依存（自動判定のバイアス・一貫性）。外部人手評価の再検証が望ましい。
* **プライバシー**：$u_t$（社会属性・自己記述）を使う。**データ最小化・匿名化**や**差分プライバシ**等の併用が実運用では必要。
* **公平性**：P-RLHF自体は少数派圧迫を緩和するが、**学習データの分布偏り**が残れば新たな歪みも—**少数派の表現強化**と**監査**が必要（Lemma 3.3の観点）。
* **コールドスタート**：未知ユーザは**e0**のみで非個人化応答→**短いアンケートや数回の比較**で早期に $e_u$ を更新する設計が実用上有効。
* **設計選択**：Individualized vs Cluster。ユーザ数が多い現場では**クラスタ型**がパラメトリックに有利（低ランク近似）。
* **ハイパラ依存**：$\alpha$ と $T_u$（ユーザトークン長）は**寄与が大**。付録のアブレーション参照。



# 論文の主張まとめ

* **フレームワークの射程**：P-RLHFは**RLHFの個人化拡張**であり、**損失側（DPO/IPO…）にも報酬側（RM）にも適用可**。
* **明示×暗黙の統合**：ユーザが語れない“暗黙”嗜好も**比較フィードバックから推定**し、**明示テキストと結合**して使う。
* **スケール効率**：ユーザ数が増えても **軽量（Nu≪Nl）** で拡張可能。
* **エビデンス**：合成・指示追従・PRISMの3系で**勝率・精度**の優位を確認。


# おわりに

P-RLHFは、“**ユーザ一様性**”というこれまで見落とされがちだった前提を解体し、**ユーザモデル＋P-DPO**という最小拡張で**個人化整合**を実現する実践的な設計です。特に、**明示情報が乏しい現場**でも**暗黙嗜好**を拾える点と、**クラスタ低ランク化**で**大規模ユーザ**に耐える点が、実運用で効きます。


# 参考（論文情報）

- **タイトル**：*ERSONALIZED LANGUAGE MODELING FROMPERSONALIZED HUMAN FEEDBACK*
- **著者**：Xinyu Li, Ruiyang Zhou, Zachary C. Lipton, and Liu Leqi
- **年**：2024
- **arXiv**： [2402.05133v3](https://arxiv.org/abs/2402.05133v3)