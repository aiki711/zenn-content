---
title: "BigFiveに基づく対話データセットを使ったLLMの性格傾向学習に関する論文を一緒に読みましょう！"
emoji: "😺"
type: "idea" # tech: 技術記事 / idea: アイデア
topics: ["LLM","NLP","Alignment","Personality","DPO"]
published: True
---

# BIG5-CHAT: 人間由来データでLLMのパーソナリティを“訓練で”形づくる
> この記事は，「自分の理解を深めたい」という気持ちで書いています．読者のみなさんと**同じ目線**で，一緒に理解を育てていくスタイルです．僕の理解が及ばない部分があれば，優しく教えていただけると幸いです！



# TL;DR
- **何をした？** Big Five（ビッグファイブ）に基づく**10万件**の対話データセット *BIG5-CHAT* を作成し．SFTとDPOで**性格傾向を訓練で付与**．プロンプト注入より妥当で人間らしい特性表現を実現． 
- **何が効いた？** 特にSFTが有効．**高い勤勉性・協調性，低い外向性・神経症傾向**のモデルは推論系ベンチで好成績．心理学の知見とも概ね整合．
- **なぜ重要？** 「その人らしさ」を**行動文の指示ではなく**，人間由来の言語パターンから学習．性格×性能の関係性も可視化し，**タスク適合な“性格チューニング”** の道を開く． 



# 背景と課題設定
従来は「あなたはパーティーの主役です」のような**行動記述をプロンプトに入れる**方式が主流だったが，LLMにとって非現実的（“パーティーに行かない”）かつ**評価妥当性**にも課題があった．大規模な**人間生成データ×性格注釈**の不足も，学習ベース手法の探索を阻んでいた． 
また，性格特性が注釈付けされた大規模な人間生成データセットが不足しているため，学習ベースのアプローチの探索が妨げられていた．
役割ベースのプロンプトがLLMの推論パターンを不均衡に，または過度に制約する可能性がある．


# 研究の貢献
1. **BIG5-CHAT**: ビッグファイブ5特性×高/低で**10万対話**（各特性2万，H/L半々）．SODAの多様な社会場面を土台に，同一コンテキストで**性格レベルのみを切り替えた**ペア対話を構築．
2. **学習での性格付与**: **SFT**と**DPO**を適用し，**プロンプト不要**で性格パターンを生成に反映．BFI/IPIP-NEO評価でプロンプトより良好な相関を獲得． 
3. **性格と推論性能の関係**: 勤勉性/協調性↑，外向性/神経症傾向↓のモデルが**推論系ベンチで高成績**．心理学の知見とも概ね一致．



# Facebook投稿データでの **エキスパート生成器** 学習

* 目的：各 Big Five 特性（O, C, E, A, N）ごとに「**High/Low の言語傾向**」を強く学習した **エキスパート生成器（LLM）** を作る．
* 役割：推論時に **Base（汎用LLM）** のロジットと合成して，望む特性へ“舵取り”する（= **DExperts**）．
* 前提：Facebook投稿と Big Five 連続スコア（0–1 等）がペアである，利用許諾・匿名化済み．

* **Alpaca形式テンプレ**（“最初の5語→残りを補完”）

   * Instruction: `Help me complete the sentence with Big Five: {trait}-{level}.`
   * Input: 投稿テキストの **先頭5語**（無ければスキップ）
   * Output: 残り全文

これにより，**特性レベル**を明示しつつ，人間文の“続き”を再現する学習になる．

> 他の特性（O/C/A/N）は，同様に各 High/Low データで **別モデル**を学習．
> **DExpertsは学習ではなく，推論フック**．**最初の5トークンはBase**→以降は合成．




# データセットの作り方（BIG5-CHAT）

* **ベース：SODA の抽出（10,000件）**

  * ランダム抽出（重複なし）．各シナリオは「話者X→話者Y」の**1ターン応答**を想定（Xの発話＝**文脈**，Yの発話＝**生成対象**）．
  * 保存するID：`scenario_id`（SODAの行），`turn_id`（X→Yの応答ペア）．

* **性格注入：PSYCHSTEER（DExperts）**

  * 構成：**Base**（LLaMA-3-70B-Instruct）と，特性別**Expert**（LLaMA-3-8B をSFT）を同時に前向き計算．
  * 生成時に各トークンでロジット合成：

    $$
    z^{\text{comb}}_t = z^{\text{base}}_t \;+\; \gamma \, z^{\text{expert}}_t
    $$

    ここで $\gamma$ は“舵の強さ”（既定：**0.5**）．$\gamma=0$ は素のBase，$\gamma$ を上げるほど特性の色が濃くなる（上げすぎは不自然化のリスク）．
  * **5語ウォームアップ**：最初の5トークンは Base のみで生成→以降は上式で合成．こうすることで**文脈の自然さ**と**特性の一貫性**を両立．

* **対話生成：同一文脈で High / Low を対に**

  * 各特性 $T \in \{O,C,E,A,N\}$ について，同一コンテキスト（話者Xの発話）に対し **High** と **Low** の2応答を作る（**特性レベルだけ**を変える）．
  * デコーディング：**greedy**（温度0相当）．出力の長さ上限・終端記号で停止．
  * 合計：**100,000**応答（= 5特性 × High/Low × 各1万シナリオ）．
  * 生成ログとメタ：`scenario_id, trait, level, gamma, decoding, base_model, expert_model, seed, pair_id` を保存．



# 手法のポイント

* **DExperts系の誘導（なぜ効く？）**

  * Expert は「特性らしい**語彙・言い回し**」の事前分布を持つ．
  * ロジット合成は「**文脈整合性（Base）** × **特性らしさ（Expert）**」の折衷．
  * **5語ウォームアップ**は“文脈の土台”を固めるための簡易ヒューリスティクス．実測でも，いきなり合成するより**破綻率が下がり，特性の可読差分が出やすい**．
  * 実務Tip：$\gamma$ を **0.3/0.5/0.7** あたりで**アブレーション**し，(1) 自然さ，(2) High/Lowの距離，(3) 誤用（文脈と不一致）の増加率を見比べる．

* **学習レシピ（Expert SFT → 本体アラインメント）**

  1. **Expert SFT（Alpaca形式）**

     * **Instruction**：`Help me complete the sentence with Big Five: {trait}-{level}`
     * **Input**：人間投稿の**先頭5語**
     * **Output**：残り（教師）
     * **Loss**：トークンCE．**目的は“外付け舵”の獲得**（推論時にBaseへ合成する素材を出す）．
  2. **BIG5-CHAT で本体70Bをアライン**

     * **SFT**：`You are a person with {trait}-{level}.`＋コンテキスト → Gold応答にCEで追従．**特性スタイルを内在化**．
     * **DPO**：同一文脈の **High=正例 / Low=負例**（または逆）をペアにして**好み最適化**．**微妙な言い回しの差**まで押し分けられる．



# 評価設定と主な結果
- **パーソナリティ妥当性**: BFI / IPIP-NEOで**訓練ベース（SFT/DPO）がプロンプトを上回る**． 
- **タスク性能（LLaMA-3系）**:  
  - **70B**: 平均的に**SFTが最良**．勤勉性・協調性が**高い**，外向性・神経症傾向が**低い**ほど**社会/数理/常識推論**で有利．  
  - **8B**: ベンチ種別ごとの挙動を詳細報告（表13）．数学では**高い開放性×DPO**が効く一方，常識推論では一貫しないなど**タスク依存性**も観測．

> まとめると，**“性格のチューニング”が（少なくとも一部の特性では）推論系能力にも波及**し得る．ただし**開放性は数学以外では限定的**という**領域特異性**も示唆． 



# 考察：なぜ“学習ベース”が効くのか
- **行動記述ではなく言語パターン**を学習するため，**テキスト内の一貫した語用論・語彙選好**が再現されやすい．  
- **プロンプト注入の過剰・非現実性**を回避し，**評価（BFI/IPIP-NEO）との整合**が高まる． 
- **性能影響のヒント**: 勤勉性・協調性は**系統立て方/衝突回避**に効き，外向性・神経症の低さは**注意散漫や不安定さの抑制**として現れる可能性．実験結果は心理学のメタ知見とも概ね整合．



# 倫理・限界・今後
- **バイアス継承のリスク**: 人間生成データに由来する**社会的ステレオタイプ**や**不適切行動**の学習可能性．論文でも**特定特性のシナリオ出力に偏り**の例示あり． 
- **評価カバレッジ**: 本研究の**性格×推論**分析は**限られたタスク文脈**に依存．より広範な設定での一般化検証が必要． 
- **単一特性の前提**: 本稿は**単一特性の独立操縦**にフォーカス．実際の人格は**多特性の相互作用**で発現するため，**マルチトレイト操縦**（複数エキスパートの合成や結合学習）は今後の重要テーマ．  
- **フレームワーク自体の限界**: ビッグファイブは広く妥当だが**全ての文化・集団の多様性を網羅しない**．学習元データの**年齢/英語/ネット利用**などの偏りも指摘． 



# まとめ
- **BIG5-CHAT**は，人間らしい性格表現を言語から学習させるための**実用的コーパス**．  
- **SFT/DPOによる学習ベースの性格付与**は，プロンプトよりも**妥当かつ一貫**．一部特性では**推論性能の向上**も確認．
- ただし**倫理・分布偏り・評価範囲**には注意．**多特性の合成**と**広範な一般化検証**が次の課題．



# 参考（論文情報）

- **タイトル**：*BIG5-CHAT: Shaping LLM PersonalitiesThrough Training on Human-Grounded Data*
- **著者**：Wenkai Li, Jiarui Liu, Andy Liu, Xuhui Zhou, Mona Diab, Maarten Sap
- **年**：2025
- **arXiv**： [2410.16491v2](https://arxiv.org/abs/2410.16491v2)